# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16MHPCgS3g85SBVLBzB2_yWo9Hwumio34
"""

from transformers import  AutoTokenizer
import pandas as pd
import numpy as np
import torch
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/ML/EmoTrain.csv'
train_df = pd.read_csv(file_path,header = 0)

"""Due to large training dataset we do stratified sampling from training dataset"""

train_df

train_df = train_df.drop(columns=['Unnamed: 0'], errors='ignore')

print(train_df.dtypes)

train_df['label_sum'] = train_df.iloc[:, 1:].sum(axis=1)  # Add a temporary column for stratification
train_df

print(train_df['label_sum'].value_counts())  # Count occurrences of each sum

# Remove rows where 'label_sum' appears only once
valid_sums = train_df['label_sum'].value_counts()
valid_sums = valid_sums[valid_sums > 1].index  # Keep only values with at least 2 samples
train_df = train_df[train_df['label_sum'].isin(valid_sums)]

sample_size = 60000

# Stratified sampling
sampled_train_df, _ = train_test_split(
    train_df,
    train_size=sample_size,
    stratify=train_df['label_sum'],  # Stratify based on the sum of labels
    random_state=42
)

# Drop the temporary column after sampling
sampled_train_df = sampled_train_df.drop(columns=['label_sum'])

# Replace train_df with the sampled subset
train_df = sampled_train_df

#train_df = train_df.sample(frac=0.1, random_state=42)

file_path = '/content/drive/MyDrive/ML/EmoVal.csv'
validate_df = pd.read_csv(file_path,header = 0)

#train_df = pd.read_csv('EmoTrain.csv',header = 0)
#validate_df = pd.read_csv('EmoVal.csv',header = 0)

labels = list(train_df.columns[1:])
train_texts = train_df['text'].tolist()  # Extract texts from train set
train_labels = train_df.iloc[:, 1:].values

import matplotlib.pyplot as plt
label_frequencies = (train_df.iloc[:, 1:].sum()/len(train_df))*100  # Assuming column 0 is not a label column

# Plot the frequencies as a bar chart
plt.figure(figsize=(10, 6))
label_frequencies.plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Frequency of Each Label in the Dataset', fontsize=16)
plt.xlabel('Labels', fontsize=14)
plt.ylabel('Frequency (Count of 1s)', fontsize=14)
plt.xticks(rotation=45, fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Display the plot
plt.show()

import matplotlib.pyplot as plt
label_frequencies = (train_df.iloc[:, 1:].sum()/len(train_df))*100  # Assuming column 0 is not a label column

# Plot the frequencies as a bar chart
plt.figure(figsize=(10, 6))
label_frequencies.plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Frequency of Each Label in the Dataset', fontsize=16)
plt.xlabel('Labels', fontsize=14)
plt.ylabel('Frequency (Count of 1s)', fontsize=14)
plt.xticks(rotation=45, fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Display the plot
plt.show()

train_texts[10]

labels

id2label = {idx:label for idx, label in enumerate(labels)}
label2id = label2id = {label:idx for idx, label in enumerate(labels)}
id2label
label2id

validate_df = validate_df.drop(columns=['Unnamed: 0'], errors='ignore')
val_texts = validate_df['text'].tolist()  # Extract texts from validation set
val_labels = validate_df.iloc[:, 1:].values

train_df['text']

!pip install datasets

from datasets import Dataset
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(validate_df)

# Inspect the dataset
print(train_dataset)
print(val_dataset)

!pip install emoji

import emoji
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def preprocess_data(examples):
    # Extract the "text" column for tokenization
    examples["text"] = [emoji.demojize(text) for text in examples["text"]]
    text = examples['text']

    # Tokenize the text
    encoding = tokenizer(text, padding="max_length", truncation=True, max_length=128)

    # Process labels
    labels_matrix = np.zeros((len(text), len(labels)))  # Shape: (batch_size, num_labels)
    for idx, label in enumerate(labels):
        labels_matrix[:, idx] = examples[label]  # Extract label columns

    encoding["labels"] = labels_matrix.tolist()

    return encoding

# Preprocess the training dataset
encoded_train = train_dataset.map(preprocess_data, batched=True, remove_columns=train_dataset.column_names)

# Preprocess the validation dataset
encoded_val = val_dataset.map(preprocess_data, batched=True, remove_columns=val_dataset.column_names)

# Inspect the processed dataset
print(encoded_train)
print(encoded_val)

encoded_train['labels']

example = encoded_train['input_ids'][0]
print(example)

tokenizer.decode(example)

encoded_train.set_format("torch")
encoded_val.set_format("torch")

encoded_train._format_type

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased",
                                                           problem_type="multi_label_classification",
                                                           num_labels=len(labels),
                                                           id2label=id2label,
                                                           label2id=label2id)

batch_size = 16
metric_name = "f1"

from transformers import TrainingArguments, Trainer

args = TrainingArguments(
    f"distilbert-finetuned",
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    report_to="none",
    metric_for_best_model=metric_name,
    #push_to_hub=True,
)

from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score
import torch
import numpy as np

def multi_label_metrics(predictions, labels, threshold=0.5):
    """Computes multi-label classification metrics."""
    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(torch.Tensor(predictions))

    # Apply threshold to obtain binary labels
    y_pred = (probs >= threshold).numpy()
    y_true = labels

    # Compute micro and weighted metrics
    f1 = f1_score(y_true, y_pred, average='micro')
    f1_weighted = f1_score(y_true, y_pred, average='weighted')  # ✅ Added weighted F1
    precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)
    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)  # ✅ Added weighted Precision
    recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)
    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)  # ✅ Added weighted Recall
    roc_auc = roc_auc_score(y_true, y_pred, average='micro')
    accuracy = accuracy_score(y_true, y_pred)

    return {
        "f1": f1,
        "f1_weighted": f1_weighted,  # ✅ Now available
        "precision_micro": precision_micro,
        "precision_weighted": precision_weighted,  # ✅ Now available
        "recall_micro": recall_micro,
        "recall_weighted": recall_weighted,  # ✅ Now available
        "roc_auc": roc_auc,
        "accuracy": accuracy
    }

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions,
            tuple) else p.predictions
    result = multi_label_metrics(
        predictions=preds,
        labels=p.label_ids)
    return result

trainer = Trainer(
    model,
    args,
    train_dataset=encoded_train,
    eval_dataset=encoded_val,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.evaluate()

text = "criterion is such a great company though. i'm realy hoping they got they new streaming service up and running soon. "

text = emoji.demojize(text)
encoding = tokenizer(text, return_tensors="pt")
encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}

outputs = trainer.model(**encoding)

encoding.items()

logits = outputs.logits
logits.shape

# apply sigmoid + threshold
sigmoid = torch.nn.Sigmoid()
probs = sigmoid(logits.squeeze().cpu())
predictions = np.zeros(probs.shape)
predictions[np.where(probs >= 0.5)] = 1
# turn predicted id's into actual label names
predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]
print(predicted_labels)
probs

import os
output_dir = "/content/drive/MyDrive/first_model"

first_model_dir = os.path.join(output_dir, "first_model")

# Save the final model
model.save_pretrained(first_model_dir)
tokenizer.save_pretrained(first_model_dir)
print("First model saved at:", first_model_dir)

from google.colab import drive
import json

# Mount Google Drive
#drive.mount('/content/drive')
# Load model configuration
config = model.config
# Save hyperparameters to a JSON file
hyperparams = {
    "model_name": "distilbert-base-uncased",
    "num_labels": config.num_labels,
    "hidden_dropout": getattr(config, "hidden_dropout_prob", "N/A"),
    "attention_dropout": getattr(config, "attention_probs_dropout_prob", "N/A"),
    "batch_size_train": args.per_device_train_batch_size,
    "batch_size_eval": args.per_device_eval_batch_size,
    "learning_rate": args.learning_rate,
    "epochs": args.num_train_epochs,
    "weight_decay": args.weight_decay,
    "save_strategy": args.save_strategy,
    "metric_for_best_model": args.metric_for_best_model
}

save_path = "/content/drive/MyDrive/first_hyperparameters.json"

with open(save_path, "w") as f:
    json.dump(hyperparams, f, indent=4)

print(f"✅ Hyperparameters saved to {save_path}")

#!zip -r first_model.zip first_model

# Download the zip file
#from google.colab import files
#files.download("first_model.zip")

"""**Now we want to perform hyperparameter search**

Due to large training dataset we do stratified sampling from training dataset
"""

encoded_val

encoded_train

"""**Model Init**"""

import os
output_dir = "/content/drive/MyDrive/hyperparameter_search_models"
os.makedirs(output_dir, exist_ok=True)

from transformers import AutoConfig, AutoModelForSequenceClassification
def model_init(trial=None):
    """Initializes the model with hyperparameters from the trial if provided"""

    # Use best hyperparameters if a trial is provided, else use default values
    dropout = trial.suggest_float("dropout", 0.1, 0.5) if trial else 0.1  # Default dropout = 0.1

    # Load model config with correct dropout settings
    config = AutoConfig.from_pretrained(
        "distilbert-base-uncased",
        problem_type="multi_label_classification",
        num_labels=len(labels),
        id2label=id2label,
        label2id=label2id,
        hidden_dropout=dropout,  # ✅ Correct way to set dropout for DistilBERT
    )

    return AutoModelForSequenceClassification.from_pretrained(
        "distilbert-base-uncased",
        config=config  # ✅ Use config instead of passing dropout as kwargs
    )

def hp_space_optuna(trial):
    return {
        "learning_rate": trial.suggest_float("learning_rate", 1e-5, 5e-4, log=True),
        "per_device_train_batch_size": trial.suggest_categorical("per_device_train_batch_size", [32, 64]),
        "weight_decay": trial.suggest_float("weight_decay", 0.01, 0.1),
        "num_train_epochs": trial.suggest_categorical("num_train_epochs", [3, 5, 10]),
        "warmup_steps": trial.suggest_int("warmup_steps", 0, 500),
        "adam_beta1": trial.suggest_float("adam_beta1", 0.8, 0.99),
        "adam_beta2": trial.suggest_float("adam_beta2", 0.9, 0.999),
        "label_smoothing_factor": trial.suggest_float("label_smoothing_factor", 0.0, 0.2),
    }

args = TrainingArguments(
    output_dir="/content/drive/MyDrive/hyperparameter_search_models",
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,
    logging_dir="./logs",
    logging_steps=100,
    report_to="none",
    save_total_limit=1,
)

from transformers import EarlyStoppingCallback

trainer = Trainer(
    model_init=model_init,
    args=args,
    train_dataset=encoded_train,
    eval_dataset=encoded_val,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

!pip install optuna

best_run = trainer.hyperparameter_search(
    direction="maximize",
    hp_space=hp_space_optuna,  # Search space definition
    backend="optuna",         # You can replace with "ray" or "sigopt"
    n_trials=10,              # Number of trials
)

# Print the best hyperparameters
print("Best hyperparameters:", best_run.hyperparameters)

import json

save_dir = "/content/drive/MyDrive/hyperparameter_search_models"
# Save best hyperparameters to JSON file
best_hyperparams_file = os.path.join(save_dir, "best_hyperparameters.json")
with open(best_hyperparams_file, "w") as f:
    json.dump(best_run.hyperparameters, f)

print(f"Best hyperparameters saved in Google Drive: {best_hyperparams_file}")

best_model_dir = os.path.join(output_dir, "best_model")

# Train the model with the best hyperparameters
for param, value in best_run.hyperparameters.items():
    setattr(trainer.args, param, value)
trainer.train()

"""save best model on drive"""

trainer.save_model(best_model_dir)
tokenizer.save_pretrained(best_model_dir)
print("Best model saved at:", best_model_dir)

eval_results = trainer.evaluate()

eval_results

hyper_trainer = trainer

"""**Test on single text**"""

text = "criterion is such a great company though. i'm realy hoping they got they new streaming service up and running soon. "

text = emoji.demojize(text)
encoding = tokenizer(text, return_tensors="pt")
encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}

outputs = trainer.model(**encoding)

# apply sigmoid + threshold
sigmoid = torch.nn.Sigmoid()
probs = sigmoid(logits.squeeze().cpu())
predictions = np.zeros(probs.shape)
predictions[np.where(probs >= 0.5)] = 1
# turn predicted id's into actual label names
predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]
print(predicted_labels)
probs

"""**Train on the full dataset with hyperparameters found in search**"""

file_path = '/content/drive/MyDrive/ML/EmoTrain.csv'
train_df = pd.read_csv(file_path,header = 0)

train_df = train_df.drop(columns=['Unnamed: 0'], errors='ignore')

train_df['text']

train_dataset = Dataset.from_pandas(train_df)

encoded_train = train_dataset.map(preprocess_data, batched=True, remove_columns=train_dataset.column_names)

encoded_train.set_format("torch")

encoded_train['labels']

# Apply best hyperparameters
for param, value in best_run.hyperparameters.items():
    setattr(args, param, value)
args.label_smoothing_factor = 0.0

trainer = Trainer(
    model_init=model_init,
    args=args,
    train_dataset=encoded_train,  # FULL DATASET NOW
    eval_dataset=encoded_val,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

output_dir = "/content/drive/MyDrive/final_emotion_model"
os.makedirs(output_dir, exist_ok=True)

trainer.save_model(output_dir)  # Save model
tokenizer.save_pretrained(output_dir)  # Save tokenizer
print(f"Final model saved at {output_dir}")

trainer.evaluate()

"""**Test on single text**"""

text = "criterion is such a great company though. i'm realy hoping they got they new streaming service up and running soon. "

text = emoji.demojize(text)
encoding = tokenizer(text, return_tensors="pt")
encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}

outputs = trainer.model(**encoding)

# apply sigmoid + threshold
sigmoid = torch.nn.Sigmoid()
probs = sigmoid(logits.squeeze().cpu())
predictions = np.zeros(probs.shape)
predictions[np.where(probs >= 0.5)] = 1
# turn predicted id's into actual label names
predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]
print(predicted_labels)
probs

"""**Cross validation using best hyperparameters**"""

import pandas as pd

# Load full train and validation datasets
train_df = pd.read_csv('/content/drive/MyDrive/ML/EmoTrain.csv', header=0)
validate_df = pd.read_csv('/content/drive/MyDrive/ML/EmoVal.csv', header=0)

# Drop unnecessary columns if they exist
train_df = train_df.drop(columns=['Unnamed: 0'], errors='ignore')
validate_df = validate_df.drop(columns=['Unnamed: 0'], errors='ignore')

# Merge both datasets
full_df = pd.concat([train_df, validate_df], axis=0).reset_index(drop=True)

# Print dataset size
print(f"Total dataset size after merging: {full_df.shape[0]} samples")

label_cols = list(full_df.columns[1:])    # all columns except "text"
num_labels = len(label_cols)

# 'X' = texts
X = full_df["text"].to_numpy()

# 'y' = 2D NumPy array of shape (num_samples, num_labels)
y = full_df.iloc[:, 1:].to_numpy()

# ----------------------
# 3) Build id2label, label2id
# ----------------------
id2label = {i: lbl for i, lbl in enumerate(label_cols)}
label2id = {lbl: i for i, lbl in enumerate(label_cols)}

# Make sure your preprocess_data function references a global "labels"
# or uses 'label_cols' directly. For example:
labels = label_cols

X

k = 5
skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)

# For storing results
f1_micro_scores = []
f1_weighted_scores = []
accuracy_scores = []
roc_auc_scores = []
precision_micro_scores = []
precision_weighted_scores = []
recall_micro_scores = []
recall_weighted_scores = []

# If you have best_run from hyperparameter search
# it has best_run.hyperparameters dictionary, e.g.:
# {
#   "learning_rate": 1.23e-5,
#   "per_device_train_batch_size": 32,
#   "weight_decay": 0.05,
#   "num_train_epochs": 5,
#   ...
# }
# Make sure label_smoothing_factor = 0.0 for multi-label BCE

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y.sum(axis=1))):
    print(f"\n===== Fold {fold+1}/{k} =====")

    # ----------------------
    # 4A) Build fold DataFrames
    # ----------------------
    train_texts = X[train_idx]
    val_texts   = X[val_idx]
    train_labels = y[train_idx]
    val_labels   = y[val_idx]

    train_fold_df = pd.DataFrame({"text": train_texts})
    val_fold_df   = pd.DataFrame({"text": val_texts})

    # Add each label column
    for i, col_name in enumerate(label_cols):
        train_fold_df[col_name] = train_labels[:, i]
        val_fold_df[col_name]   = val_labels[:, i]

    # ----------------------
    # 4B) Convert to Hugging Face Datasets
    # ----------------------
    train_dataset = Dataset.from_pandas(train_fold_df)
    val_dataset   = Dataset.from_pandas(val_fold_df)

    print("Dataset columns before preprocessing:", train_dataset.column_names)

    train_dataset = train_dataset.map(
        preprocess_data,
        batched=True,
        remove_columns=train_dataset.column_names
    )
    val_dataset = val_dataset.map(
        preprocess_data,
        batched=True,
        remove_columns=val_dataset.column_names
    )

    # Set format for PyTorch
    train_dataset.set_format("torch")
    val_dataset.set_format("torch")

    # ----------------------
    # 4C) Initialize fresh model
    # ----------------------
    model = AutoModelForSequenceClassification.from_pretrained(
        "distilbert-base-uncased",
        problem_type="multi_label_classification",
        num_labels=num_labels,
        id2label=id2label,
        label2id=label2id
    )

    # ----------------------
    # 4D) TrainingArguments
    # ----------------------
    args = TrainingArguments(
        output_dir=f"/content/drive/MyDrive/cv_models/fold-{fold+1}",
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=best_run.hyperparameters["learning_rate"],
        per_device_train_batch_size=int(best_run.hyperparameters["per_device_train_batch_size"]),
        per_device_eval_batch_size=int(best_run.hyperparameters["per_device_train_batch_size"]),
        num_train_epochs=int(best_run.hyperparameters["num_train_epochs"]),
        weight_decay=best_run.hyperparameters["weight_decay"],
        load_best_model_at_end=True,
        metric_for_best_model="f1",  # If your compute_metrics returns "f1" as the main key
        greater_is_better=True,
        report_to="none",
        logging_dir=f"./logs/fold-{fold+1}",
        save_total_limit=1
    )

    # ----------------------
    # 4E) Define Trainer
    # ----------------------
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,  # <--- per-fold datasets
        eval_dataset=val_dataset,     # <--- per-fold datasets
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    # ----------------------
    # 4F) Train & Evaluate
    # ----------------------
    trainer.train()
    results = trainer.evaluate()

    f1_micro_scores.append(results["eval_f1"])
    f1_weighted_scores.append(results["eval_f1_weighted"])
    accuracy_scores.append(results["eval_accuracy"])
    roc_auc_scores.append(results["eval_roc_auc"])
    precision_micro_scores.append(results["eval_precision_micro"])
    precision_weighted_scores.append(results["eval_precision_weighted"])
    recall_micro_scores.append(results["eval_recall_micro"])
    recall_weighted_scores.append(results["eval_recall_weighted"])

    print(f"\nFold {fold+1} Scores:")
    print(f"  F1 (micro):          {results['eval_f1']:.4f}")
    print(f"  F1 (weighted):       {results['eval_f1_weighted']:.4f}")
    print(f"  Accuracy:            {results['eval_accuracy']:.4f}")
    print(f"  AUC-ROC:             {results['eval_roc_auc']:.4f}")
    print(f"  Precision (micro):   {results['eval_precision_micro']:.4f}")
    print(f"  Precision (weighted):{results['eval_precision_weighted']:.4f}")
    print(f"  Recall (micro):      {results['eval_recall_micro']:.4f}")
    print(f"  Recall (weighted):   {results['eval_recall_weighted']:.4f}")

# ----------------------
# 5) Print Overall CV Results
# ----------------------
print("\n===== Cross-Validation Results =====")
print(f"F1 Micro:     {np.mean(f1_micro_scores):.4f} ± {np.std(f1_micro_scores):.4f}")
print(f"F1 Weighted:  {np.mean(f1_weighted_scores):.4f} ± {np.std(f1_weighted_scores):.4f}")
print(f"Accuracy:     {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}")
print(f"AUC-ROC:      {np.mean(roc_auc_scores):.4f} ± {np.std(roc_auc_scores):.4f}")
print(f"Precision Micro:   {np.mean(precision_micro_scores):.4f} ± {np.std(precision_micro_scores):.4f}")
print(f"Precision Weighted:{np.mean(precision_weighted_scores):.4f} ± {np.std(precision_weighted_scores):.4f}")
print(f"Recall Micro:      {np.mean(recall_micro_scores):.4f} ± {np.std(recall_micro_scores):.4f}")
print(f"Recall Weighted:   {np.mean(recall_weighted_scores):.4f} ± {np.std(recall_weighted_scores):.4f}")

"""**load model**"""



"""**Load Hyperparameters**"""



"""**Output of DistillBert for inputs**"""



"""**MLP**"""

